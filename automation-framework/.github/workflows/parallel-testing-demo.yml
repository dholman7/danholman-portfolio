name: Parallel Testing Demo

on:
  workflow_dispatch:
    inputs:
      test_scope:
        type: choice
        options:
          - all
          - unit
          - integration
          - e2e
          - performance
          - api
          - ui
        required: true
        default: all
        description: Test scope to run
      max_parallel:
        type: string
        required: false
        default: "5"
        description: Maximum parallel jobs (1-10)
      environment:
        type: choice
        options:
          - staging
          - production
        required: true
        default: staging
        description: Test environment

jobs:
  # Generate test matrix dynamically
  generate-matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
      matrix-size: ${{ steps.set-matrix.outputs.matrix-size }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.13
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install poetry
          poetry install --no-root
      
      - name: Generate test matrix
        id: generate-matrix
        run: |
          python tests/parallel_testing_example.py \
            --scope "${{ github.event.inputs.test_scope || 'all' }}" \
            --output test_matrix.json
      
      - name: Set matrix data
        id: set-matrix
        run: |
          matrix_data=$(jq -c . < test_matrix.json)
          matrix_size=$(jq length < test_matrix.json)
          echo "matrix=${matrix_data}" >> $GITHUB_OUTPUT
          echo "matrix-size=${matrix_size}" >> $GITHUB_OUTPUT
          echo "Generated matrix with ${matrix_size} test configurations"
      
      - name: Archive matrix
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-matrix.json
          path: test_matrix.json

  # Parallel test execution
  parallel-tests:
    runs-on: ubuntu-latest
    needs: generate-matrix
    continue-on-error: true
    strategy:
      fail-fast: false
      max-parallel: ${{ fromJSON(github.event.inputs.max_parallel || '5') }}
      matrix:
        test_config: ${{ fromJSON(needs.generate-matrix.outputs.matrix) }}
    
    env:
      TEST_TYPE: ${{ matrix.test_config.test_type }}
      TEST_FRAMEWORK: ${{ matrix.test_config.framework }}
      TEST_LANGUAGE: ${{ matrix.test_config.language }}
      TEST_CATEGORY: ${{ matrix.test_config.category }}
      TEST_MODULE: ${{ matrix.test_config.module }}
      TEST_ENVIRONMENT: ${{ matrix.test_config.environment }}
      BROWSER: ${{ matrix.test_config.browser || 'chrome' }}
      DEVICE: ${{ matrix.test_config.device || 'desktop' }}
    
    steps:
      - name: Debug test configuration
        run: |
          echo "=== Test Configuration ==="
          echo "Test Type: $TEST_TYPE"
          echo "Framework: $TEST_FRAMEWORK"
          echo "Language: $TEST_LANGUAGE"
          echo "Category: $TEST_CATEGORY"
          echo "Module: $TEST_MODULE"
          echo "Environment: $TEST_ENVIRONMENT"
          echo "Browser: $BROWSER"
          echo "Device: $DEVICE"
          echo "Matrix Config: ${{ toJson(matrix.test_config) }}"
      
      - name: Set sanitized job name
        run: |
          sanitized_name=$(echo "${TEST_TYPE}_${TEST_FRAMEWORK}_${TEST_LANGUAGE}_${TEST_CATEGORY}_${TEST_ENVIRONMENT}" | tr -cs 'a-zA-Z0-9_' '_')
          if [ -n "$BROWSER" ] && [ "$BROWSER" != "null" ]; then
            sanitized_name="${sanitized_name}_${BROWSER}"
          fi
          if [ -n "$DEVICE" ] && [ "$DEVICE" != "null" ]; then
            sanitized_name="${sanitized_name}_${DEVICE}"
          fi
          echo "JOB_NAME=${sanitized_name}" >> $GITHUB_ENV
          echo "Job Name: ${sanitized_name}"
      
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.13
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"
      
      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            ~/.cache/pytest
            ~/.cache/poetry
          key: ${{ runner.os }}-${{ matrix.test_config.language }}-${{ matrix.test_config.framework }}-${{ hashFiles('**/poetry.lock') }}
          restore-keys: |
            ${{ runner.os }}-${{ matrix.test_config.language }}-${{ matrix.test_config.framework }}-
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install poetry
          poetry install --no-root
          
          # Install additional dependencies based on test type
          if [ "$TEST_TYPE" = "e2e" ]; then
            poetry add playwright
            playwright install
          fi
      
      - name: Create reports directory
        run: mkdir -p reports
      
      - name: Run tests with Allure
        run: |
          # Run tests based on configuration
          if [ "$TEST_LANGUAGE" = "python" ] && [ "$TEST_FRAMEWORK" = "pytest" ]; then
            poetry run pytest ${{ matrix.test_config.test_path }}/ \
              --html=reports/${JOB_NAME}.html \
              --self-contained-html \
              --cov=src \
              --cov-report=xml:reports/${JOB_NAME}_coverage.xml \
              --cov-report=html:reports/${JOB_NAME}_coverage.html \
              --junitxml=reports/${JOB_NAME}.xml \
              --allure-results-dir=allure-results \
              -v \
              --tb=short
          else
            echo "Running generic tests for $TEST_TYPE"
            poetry run pytest tests/ -v --tb=short
          fi
      
      - name: Generate Allure report
        if: always()
        run: |
          if [ -d "allure-results" ] && [ "$(ls -A allure-results)" ]; then
            poetry run allure generate allure-results --clean -o reports/allure-report
          else
            echo "No Allure results found"
          fi
      
      - name: List test artifacts
        run: |
          echo "Test artifacts for ${JOB_NAME}:"
          ls -la reports/ || echo "No reports directory found"
        if: always()
      
      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ env.JOB_NAME }}
          path: |
            reports/
            allure-results/
          retention-days: 30

  # Merge and aggregate test results
  merge-results:
    runs-on: ubuntu-latest
    needs: [generate-matrix, parallel-tests]
    if: always()
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.13
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"
      
      - name: Install merge dependencies
        run: |
          python -m pip install --upgrade pip
          pip install poetry pytest-html-merger junit-xml allure-python-commons
          poetry install --no-root
      
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: downloaded_artifacts
      
      - name: Display artifact structure
        run: |
          echo "Downloaded artifacts structure:"
          find downloaded_artifacts -type f -name "*.html" -o -name "*.xml" -o -name "*.json" | head -20
        if: always()
      
      - name: Merge Allure results
        run: |
          mkdir -p merged_reports/allure-results
          
          # Collect all Allure results
          find downloaded_artifacts -name "allure-results" -type d | while read dir; do
            echo "Found Allure results in: $dir"
            cp -r "$dir"/* merged_reports/allure-results/ 2>/dev/null || true
          done
          
          # Generate merged Allure report
          if [ -d "merged_reports/allure-results" ] && [ "$(ls -A merged_reports/allure-results)" ]; then
            poetry run allure generate merged_reports/allure-results --clean -o merged_reports/allure-report
            echo "Merged Allure report generated"
          else
            echo "No Allure results found to merge"
          fi
        if: always()
      
      - name: Merge HTML reports
        run: |
          mkdir -p merged_reports
          
          # Find all HTML reports
          find downloaded_artifacts -name "*.html" -not -path "*/coverage/*" -not -path "*/allure-report/*" | while read file; do
            echo "Found HTML report: $file"
          done
          
          # Merge pytest HTML reports if any exist
          if find downloaded_artifacts -name "*.html" -not -path "*/coverage/*" -not -path "*/allure-report/*" | grep -q .; then
            poetry run pytest-html-merger -i downloaded_artifacts -o merged_reports/parallel_test_results.html
            echo "Merged HTML report generated"
          else
            echo "No HTML reports found to merge"
          fi
        if: always()
      
      - name: Merge JUnit XML reports
        run: |
          mkdir -p merged_reports
          
          # Find all JUnit XML reports
          find downloaded_artifacts -name "*.xml" | while read file; do
            echo "Found XML report: $file"
          done
          
          # Create aggregated JUnit report
          python -c "
          import xml.etree.ElementTree as ET
          import os
          from pathlib import Path
          
          # Create root test suite
          root = ET.Element('testsuites')
          root.set('name', 'Parallel Test Execution')
          
          total_tests = 0
          total_failures = 0
          total_errors = 0
          total_time = 0.0
          
          # Process all XML files
          for xml_file in Path('downloaded_artifacts').rglob('*.xml'):
              if xml_file.name.endswith('_coverage.xml'):
                  continue
              try:
                  tree = ET.parse(xml_file)
                  testsuite = tree.getroot()
                  
                  # Update totals
                  total_tests += int(testsuite.get('tests', 0))
                  total_failures += int(testsuite.get('failures', 0))
                  total_errors += int(testsuite.get('errors', 0))
                  total_time += float(testsuite.get('time', 0))
                  
                  # Add to root
                  root.append(testsuite)
              except Exception as e:
                  print(f'Error processing {xml_file}: {e}')
          
          # Set root attributes
          root.set('tests', str(total_tests))
          root.set('failures', str(total_failures))
          root.set('errors', str(total_errors))
          root.set('time', str(total_time))
          
          # Write merged report
          tree = ET.ElementTree(root)
          tree.write('merged_reports/parallel_test_results.xml', encoding='utf-8', xml_declaration=True)
          print(f'Merged JUnit report with {total_tests} tests, {total_failures} failures, {total_errors} errors')
          "
        if: always()
      
      - name: Generate test summary
        run: |
          python -c "
          import json
          import os
          from pathlib import Path
          
          summary = {
              'total_jobs': 0,
              'successful_jobs': 0,
              'failed_jobs': 0,
              'total_tests': 0,
              'passed_tests': 0,
              'failed_tests': 0,
              'skipped_tests': 0,
              'test_types': {},
              'execution_summary': {
                  'matrix_size': '${{ needs.generate-matrix.outputs.matrix-size }}',
                  'max_parallel': '${{ github.event.inputs.max_parallel || '5' }}',
                  'test_scope': '${{ github.event.inputs.test_scope || 'all' }}',
                  'environment': '${{ github.event.inputs.environment || 'staging' }}'
              }
          }
          
          # Count jobs and tests
          for artifact_dir in Path('downloaded_artifacts').iterdir():
              if artifact_dir.is_dir():
                  summary['total_jobs'] += 1
                  
                  # Check for success indicators
                  if any(Path(artifact_dir).glob('*.html')):
                      summary['successful_jobs'] += 1
                  else:
                      summary['failed_jobs'] += 1
          
          # Generate markdown summary
          with open('merged_reports/test_summary.md', 'w') as f:
              f.write('# üß™ Parallel Test Execution Summary\n\n')
              f.write(f'## üìä Execution Overview\n')
              f.write(f'- **Total Jobs**: {summary[\"total_jobs\"]}\n')
              f.write(f'- **Successful Jobs**: {summary[\"successful_jobs\"]}\n')
              f.write(f'- **Failed Jobs**: {summary[\"failed_jobs\"]}\n')
              f.write(f'- **Matrix Size**: {summary[\"execution_summary\"][\"matrix_size\"]}\n')
              f.write(f'- **Max Parallel**: {summary[\"execution_summary\"][\"max_parallel\"]}\n')
              f.write(f'- **Test Scope**: {summary[\"execution_summary\"][\"test_scope\"]}\n')
              f.write(f'- **Environment**: {summary[\"execution_summary\"][\"environment\"]}\n\n')
              
              f.write('## üìÅ Generated Reports\n')
              f.write('- **Allure Report**: `allure-report/` (if available)\n')
              f.write('- **HTML Report**: `parallel_test_results.html`\n')
              f.write('- **JUnit XML**: `parallel_test_results.xml`\n')
              f.write('- **Test Summary**: `test_summary.md`\n\n')
              
              f.write('## üöÄ Next Steps\n')
              f.write('1. Review individual job artifacts for detailed results\n')
              f.write('2. Check Allure report for comprehensive test analytics\n')
              f.write('3. Analyze failed tests and update test cases as needed\n')
              f.write('4. Use parallel testing for faster CI/CD pipelines\n')
          
          print('Test summary generated')
          "
        if: always()
      
      - name: Upload merged results
        uses: actions/upload-artifact@v4
        with:
          name: parallel-test-results
          path: |
            merged_reports/
          retention-days: 90
      
      - name: Comment on PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            
            try {
              const summaryPath = 'merged_reports/test_summary.md';
              if (fs.existsSync(summaryPath)) {
                const summary = fs.readFileSync(summaryPath, 'utf8');
                
                github.rest.issues.createComment({
                  issue_number: context.issue.number,
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  body: `## üß™ Parallel Test Results\n\n${summary}`
                });
              }
            } catch (error) {
              console.log('Could not post test summary:', error.message);
            }
