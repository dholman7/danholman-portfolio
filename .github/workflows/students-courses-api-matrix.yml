name: Students & Courses API Matrix Tests

on:
  workflow_dispatch:
    inputs:
      environment:
        type: choice
        options:
          - dev
          - staging
          - prod
        required: false
        default: staging
        description: Target environment for API testing
      api_type:
        type: choice
        options:
          - all
          - students
          - courses
          - enrollments
        required: false
        default: all
        description: API type to test (all, students, courses, enrollments)
      test_scenario:
        type: choice
        options:
          - all
          - list
          - create
          - get_by_id
          - update
          - delete
          - enroll
        required: false
        default: all
        description: Test scenario to run
  push:
    branches: [ main, develop ]
    paths:
      - 'tests/api/**'
      - 'src/api/**'
      - 'students_courses_test_matrix.json'
      - '.github/workflows/students-courses-api-matrix.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'tests/api/**'
      - 'src/api/**'
      - 'students_courses_test_matrix.json'
      - '.github/workflows/students-courses-api-matrix.yml'

env:
  PYTHON_VERSION: '3.13'

jobs:
  # Load and filter test matrix
  load-matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Load and filter test matrix
        id: set-matrix
        run: |
          python << 'EOF'
          import json
          import os
          
          # Load test matrix
          with open('automation-framework/students_courses_test_matrix.json', 'r') as f:
              matrix = json.load(f)
          
          # Get filter parameters
          environment = os.getenv('ENVIRONMENT', 'staging')
          api_type = os.getenv('API_TYPE', 'all')
          test_scenario = os.getenv('TEST_SCENARIO', 'all')
          
          # Apply filters
          filtered_matrix = []
          for test in matrix:
              # Filter by environment
              if environment != 'all' and test['environment'] != environment:
                  continue
              
              # Filter by API type
              if api_type != 'all' and test['api_type'] != api_type:
                  continue
              
              # Filter by test scenario
              if test_scenario != 'all' and not test['test_scenario'].startswith(test_scenario):
                  continue
              
              filtered_matrix.append(test)
          
          # Output matrix for GitHub Actions
          print(f"matrix={json.dumps(filtered_matrix)}")
          EOF
        env:
          ENVIRONMENT: ${{ github.event.inputs.environment || 'staging' }}
          API_TYPE: ${{ github.event.inputs.api_type || 'all' }}
          TEST_SCENARIO: ${{ github.event.inputs.test_scenario || 'all' }}

  # Parallel API testing jobs
  api-matrix-test:
    runs-on: ubuntu-latest
    needs: load-matrix
    if: needs.load-matrix.outputs.matrix != '[]'
    continue-on-error: true
    strategy:
      fail-fast: false
      max-parallel: 10
      matrix:
        test_config: ${{ fromJson(needs.load-matrix.outputs.matrix) }}
    env:
      API_BASE_URL: ${{ matrix.test_config.environment == 'prod' && 'https://api.university.edu' || matrix.test_config.environment == 'staging' && 'https://staging-api.university.edu' || 'https://dev-api.university.edu' }}
      API_TYPE: ${{ matrix.test_config.api_type }}
      ENDPOINT: ${{ matrix.test_config.endpoint }}
      METHOD: ${{ matrix.test_config.method }}
      TEST_SCENARIO: ${{ matrix.test_config.test_scenario }}
      EXPECTED_STATUS: ${{ matrix.test_config.expected_status }}
      ENVIRONMENT: ${{ matrix.test_config.environment }}
      JOB_NAME: ${{ matrix.test_config.job_name }}
    steps:
      - name: Debug Environment Variables
        run: |
          echo "API_BASE_URL: $API_BASE_URL"
          echo "API_TYPE: $API_TYPE"
          echo "ENDPOINT: $ENDPOINT"
          echo "METHOD: $METHOD"
          echo "TEST_SCENARIO: $TEST_SCENARIO"
          echo "EXPECTED_STATUS: $EXPECTED_STATUS"
          echo "ENVIRONMENT: $ENVIRONMENT"
          echo "JOB_NAME: $JOB_NAME"
      
      - name: Set Sanitized Job Name
        run: |
          sanitized_name=$(echo "$JOB_NAME" | tr -cs 'a-zA-Z0-9_-' '_')
          echo "SANITIZED_NAME=${sanitized_name}" >> $GITHUB_ENV
      
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest-html pytest-xdist allure-pytest
      
      - name: Create test directories
        run: |
          mkdir -p reports
          mkdir -p logs
          mkdir -p test_data
      
      - name: Run API Test
        run: |
          pytest tests/api/test_students_courses_api.py \
            --html=reports/${SANITIZED_NAME}.html \
            --self-contained-html \
            --junitxml=reports/${SANITIZED_NAME}.xml \
            --allure-results-dir=reports/allure-results \
            -v \
            --tb=short \
            --strict-markers \
            -m "api and ${API_TYPE} and ${TEST_SCENARIO}"
        env:
          API_BASE_URL: ${{ env.API_BASE_URL }}
          API_TYPE: ${{ env.API_TYPE }}
          ENDPOINT: ${{ env.ENDPOINT }}
          METHOD: ${{ env.METHOD }}
          TEST_SCENARIO: ${{ env.TEST_SCENARIO }}
          EXPECTED_STATUS: ${{ env.EXPECTED_STATUS }}
          ENVIRONMENT: ${{ env.ENVIRONMENT }}
      
      - name: List test artifacts
        run: |
          echo "=== Reports Directory ==="
          ls -la reports/
          echo "=== Logs Directory ==="
          ls -la logs/
        if: always()
      
      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: ${{ env.SANITIZED_NAME }}_results
          path: |
            reports/${SANITIZED_NAME}.html
            reports/${SANITIZED_NAME}.xml
            reports/allure-results/
            logs/
          retention-days: 30

  # Merge and consolidate test results
  merge-results:
    runs-on: ubuntu-latest
    needs: [load-matrix, api-matrix-test]
    if: always() && needs.load-matrix.outputs.matrix != '[]'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install merge dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest-html-merger allure-pytest
      
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: downloaded_artifacts
      
      - name: Create merged reports directory
        run: |
          mkdir -p merged_reports
          mkdir -p merged_reports/allure-results
      
      - name: Merge HTML reports
        run: |
          # Copy all HTML reports
          find downloaded_artifacts -name "*.html" -exec cp {} merged_reports/ \;
          
          # Merge HTML reports if pytest-html-merger is available
          if command -v pytest_html_merger &> /dev/null; then
            pytest_html_merger -i merged_reports -o merged_reports/consolidated_report.html
          else
            echo "pytest-html-merger not available, skipping HTML merge"
          fi
      
      - name: Merge Allure results
        run: |
          # Copy all allure results
          find downloaded_artifacts -name "allure-results" -type d -exec cp -r {}/. merged_reports/allure-results/ \;
          
          # Generate Allure report
          if command -v allure &> /dev/null; then
            allure generate merged_reports/allure-results -o merged_reports/allure-report --clean
          else
            echo "Allure not available, skipping Allure report generation"
          fi
      
      - name: Generate test summary
        run: |
          python << 'EOF'
          import json
          import os
          import glob
          
          # Count test results
          xml_files = glob.glob('downloaded_artifacts/**/*.xml', recursive=True)
          total_tests = 0
          total_failures = 0
          total_errors = 0
          
          for xml_file in xml_files:
              try:
                  with open(xml_file, 'r') as f:
                      content = f.read()
                      # Simple XML parsing for test counts
                      total_tests += content.count('<testcase')
                      total_failures += content.count('failures=')
                      total_errors += content.count('errors=')
              except:
                  pass
          
          # Generate summary
          summary = {
              "total_tests": total_tests,
              "total_failures": total_failures,
              "total_errors": total_errors,
              "success_rate": f"{((total_tests - total_failures - total_errors) / total_tests * 100):.2f}%" if total_tests > 0 else "0%",
              "xml_files_processed": len(xml_files)
          }
          
          with open('merged_reports/test_summary.json', 'w') as f:
              json.dump(summary, f, indent=2)
          
          print("Test Summary:")
          print(json.dumps(summary, indent=2))
          EOF
      
      - name: List merged artifacts
        run: |
          echo "=== Merged Reports Directory ==="
          ls -la merged_reports/
          echo "=== Test Summary ==="
          cat merged_reports/test_summary.json || echo "No summary available"
      
      - name: Upload consolidated results
        uses: actions/upload-artifact@v4
        with:
          name: consolidated_api_test_results
          path: |
            merged_reports/
          retention-days: 30
      
      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            try {
              const summary = JSON.parse(fs.readFileSync('merged_reports/test_summary.json', 'utf8'));
              const comment = `## ðŸ§ª API Test Results Summary
              
              **Total Tests:** ${summary.total_tests}
              **Failures:** ${summary.total_failures}
              **Errors:** ${summary.total_errors}
              **Success Rate:** ${summary.success_rate}
              
              ðŸ“Š Detailed results are available in the workflow artifacts.`;
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            } catch (error) {
              console.log('Could not generate PR comment:', error.message);
            }

  # Cleanup job
  cleanup:
    runs-on: ubuntu-latest
    needs: [load-matrix, api-matrix-test, merge-results]
    if: always()
    steps:
      - name: Cleanup temporary files
        run: |
          echo "Cleaning up temporary files..."
          # This job can be used for any cleanup tasks
